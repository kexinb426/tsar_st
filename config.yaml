# random_seed: 42

# --- File Paths ---
paths:
  input_docs: "data/input/documents.jsonl"
  evaluation_data: "data/raw/tsar2025_trialdata.jsonl"
  prompts_dir: "prompts/"
  prompt_assets_dir: "prompt_assets/"
  results_dir: "results/"
  scored_references: "data/references_with_scores.jsonl"
  source_difficulty_data: "data/source_with_difficulty.jsonl"
  source_analysis_data: "data/source_with_difficulty_word_lists.jsonl"

# --- Model Definitions ---
# Define the models you can use.
models:
  gpt-4o:
    type: "gpt"
    api_model_name: "gpt-4o"
    temperature: 1.0
  o1:
    type: "gpt"
    api_model_name: "o1-2024-12-17"
    temperature: 1.0
  o3:
    type: "gpt"
    api_model_name: "o3-2025-04-16"
    temperature: 1.0
  
  gpt-5:
    type: "gpt"
    api_model_name: "gpt-5-2025-08-07"
    temperature: 1.0
  
  claude-3-7-sonnet:
    type: "claude"
    api_model_name: "arn:aws:bedrock:ap-southeast-2:340752822738:inference-profile/apac.anthropic.claude-3-7-sonnet-20250219-v1:0"
    temperature: 1.0
  claude-4-sonnet:
    type: "claude"
    api_model_name: "arn:aws:bedrock:ap-southeast-2:340752822738:inference-profile/apac.anthropic.claude-sonnet-4-20250514-v1:0"
    temperature: 1.0
  
  gemma-3-4b-it:
    type: "gemma"
    api_model_name: "google/gemma-3-4b-it"
    temperature: 1.0

# --- Pipelines ---
pipelines:
  # - name: "o3_ensemble"
  #   pipeline_type: "ensemble"
  #   candidate_runs: [
  #     "results/claude_chinese_german_20250822-001217",
  #     "results/o1_chinese_german_20250821-235916",
  #     "results/gpt5_chinese_german_20250821-233630",
  #     "results/o1_chinese_spanish_20250822-050913",
  #     "results/o1_chinese_spanish_20250822-070729",
  #     "results/claude4_1_shot_contrastive_confidence_20250814-231336",
  #     "results/gemma3_1_shot_contrastive_confidence_20250814-234437",
  #     "results/gpt5_1_shot_contrastive_confidence_20250814-222216",
  #     "results/o1_1_shot_contrastive_confidence_20250816-001954"
  #   ]
    # steps:
    #   - step_name: "filter_by_metrics"
    #     filter_criteria:
    #       # pre_calculate_metrics:
    #       #   # The key is the new metric name, the value is the formula
    #       #   edit_distance: "levenshtein(original, simplified)"
    #       #   length_ratio: "len(simplified) / len(original)"
    #       # # Now, use your new metrics in the ranking strategy
    #       # strategy: "top_percent"
    #       # metric: "BERTScore-Orig + MeaningBERT-Orig" # Rank by a combination of old and new scores
    #       # percentile: 35
    #   - step_name: "llm_judge_fusion"
    #     model: "o3"
    #     prompt_template: "llm_judge_fusion.txt"

  - name: "gpt5_shimada_0_shot"
    steps:
      - step_name: "simplify"
        model: "gpt-5"
        prompt_template: "shimada_0_shot.txt"
  
  - name: "claude4_shimada_0_shot"
    steps:
      - step_name: "simplify"
        model: "claude-4-sonnet"
        prompt_template: "shimada_0_shot.txt"
  
  - name: "o1_shimada_0_shot"
    steps:
      - step_name: "simplify"
        model: "o1"
        prompt_template: "shimada_0_shot.txt"

  # - name: "gpt5_chinese_spanish"
  #   steps:
  #     - step_name: "translate"
  #       model: "gpt-5"
  #       prompt_template: "translate.txt"

  #     - step_name: "translate_and_simplify"
  #       model: "gpt-5"
  #       prompt_template: "simplify_from_translation.txt"
  
  # - name: "o1_chinese_spanish"
  #   steps:
  #     - step_name: "translate"
  #       model: "o1"
  #       prompt_template: "translate.txt"

  #     - step_name: "translate_and_simplify"
  #       model: "o1"
  #       prompt_template: "simplify_from_translation.txt"
  
  # - name: "claude_chinese_spanish"
  #   steps:
  #     - step_name: "translate"
  #       model: "claude-4-sonnet"
  #       prompt_template: "translate.txt"

  #     - step_name: "translate_and_simplify"
  #       model: "claude-4-sonnet"
  #       prompt_template: "simplify_from_translation.txt"
  
  # - name: "gemma_chinese_spanish"
  #   steps:
  #     - step_name: "translate"
  #       model: "gemma-3-4b-it"
  #       prompt_template: "translate.txt"

  #     - step_name: "translate_and_simplify"
  #       model: "gemma-3-4b-it"
  #       prompt_template: "simplify_from_translation.txt"
  
