{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df5a02da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import spacy\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0936bd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# 1. Define the base paths for your project\n",
    "BASE_DIR = Path('/clwork/kexin/tsar_st/')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "RESULTS_DIR = BASE_DIR / 'results'\n",
    "OUTPUT_FILE = BASE_DIR / 'proxy_training_data.csv'\n",
    "\n",
    "# 2. Define the target variable (the real reference score we want to predict)\n",
    "# Choose between 'bertscore_f1_ref' or 'meaningbert_ref'\n",
    "TARGET_SCORE_COLUMN = 'meaningbert_ref'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fad9f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_to_dict(filepath, key_field, value_field):\n",
    "    \"\"\"Loads a .jsonl file into a dictionary for quick lookups.\"\"\"\n",
    "    data_dict = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            record = json.loads(line)\n",
    "            data_dict[record[key_field]] = record[value_field]\n",
    "    return data_dict\n",
    "\n",
    "def calculate_features(source_text, candidate_text, nlp_model):\n",
    "    \"\"\"Calculates features for a (source, candidate) pair.\"\"\"\n",
    "    # Handle potential empty strings to avoid errors\n",
    "    if not source_text or not candidate_text:\n",
    "        return {}\n",
    "\n",
    "    source_doc = nlp_model(source_text)\n",
    "    candidate_doc = nlp_model(candidate_text)\n",
    "\n",
    "    features = {\n",
    "        # Length-based features\n",
    "        'len_ratio_chars': len(candidate_text) / len(source_text),\n",
    "        'len_ratio_words': len(candidate_doc) / len(source_doc),\n",
    "        'abs_len_words': len(candidate_doc),\n",
    "        \n",
    "        # Candidate-intrinsic complexity features\n",
    "        'flesch_reading_ease': textstat.flesch_reading_ease(candidate_text),\n",
    "        'avg_syl_per_word': textstat.syllable_count(candidate_text, lang='en_US') / max(1, len(candidate_doc)),\n",
    "        'sentence_count': len(list(candidate_doc.sents)),\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83bb322d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting dataset construction...\n",
      "Loading spaCy model (this might take a moment)...\n",
      "Loading source documents...\n",
      "Found 41 potential experiment directories. Processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8dacc7a3724f1fb584c41f4beee4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Runs:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2561927/3841044571.py:27: DeprecationWarning: The 'lang' argument has been moved to 'textstat.set_lang(<lang>)'. This argument will be removed in the future.\n",
      "  'avg_syl_per_word': textstat.syllable_count(candidate_text, lang='en_US') / max(1, len(candidate_doc)),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Processing complete! Total records created: 1601\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸš€ Starting dataset construction...\")\n",
    "\n",
    "# Load the spaCy model once for efficiency\n",
    "print(\"Loading spaCy model (this might take a moment)...\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the static source documents into a dictionary\n",
    "print(\"Loading source documents...\")\n",
    "sources = load_jsonl_to_dict(\n",
    "    DATA_DIR / 'input/documents.jsonl',\n",
    "    key_field='text_id',\n",
    "    value_field='original'\n",
    ")\n",
    "\n",
    "all_records = []\n",
    "run_dirs = [d for d in RESULTS_DIR.iterdir() if d.is_dir()]\n",
    "print(f\"Found {len(run_dirs)} potential experiment directories. Processing...\")\n",
    "\n",
    "for run_dir in tqdm(run_dirs, desc=\"Processing Runs\"):\n",
    "    simplifications_file = run_dir / 'simplifications.jsonl'\n",
    "    scores_file = run_dir / 'individual_scores.jsonl'\n",
    "\n",
    "    if not simplifications_file.exists() or not scores_file.exists():\n",
    "        continue\n",
    "\n",
    "    scores_map = {json.loads(line)['text_id']: json.loads(line) for line in open(scores_file, 'r', encoding='utf-8')}\n",
    "    \n",
    "    with open(simplifications_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            candidate_data = json.loads(line)\n",
    "            text_id = candidate_data['text_id']\n",
    "\n",
    "            source_text = sources.get(text_id)\n",
    "            scores = scores_map.get(text_id)\n",
    "            candidate_text = candidate_data.get('simplified')\n",
    "\n",
    "            if not all([source_text, scores, candidate_text]):\n",
    "                continue\n",
    "            \n",
    "            target_score = scores.get(TARGET_SCORE_COLUMN)\n",
    "            if target_score is None:\n",
    "                continue\n",
    "\n",
    "            features = calculate_features(source_text, candidate_text, nlp)\n",
    "            sts_with_source = scores.get('meaningbert_orig')\n",
    "\n",
    "            record = {\n",
    "                'text_id': text_id,\n",
    "                'source': source_text,\n",
    "                'candidate': candidate_text,\n",
    "                'target_score': target_score, # Our 'y' variable\n",
    "                'sts_with_source': sts_with_source, # A powerful feature\n",
    "                **features\n",
    "            }\n",
    "            all_records.append(record)\n",
    "\n",
    "print(f\"\\nâœ… Processing complete! Total records created: {len(all_records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a9e4bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1601 entries, 0 to 1600\n",
      "Data columns (total 11 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   text_id              1601 non-null   object \n",
      " 1   source               1601 non-null   object \n",
      " 2   candidate            1601 non-null   object \n",
      " 3   target_score         1601 non-null   float64\n",
      " 4   sts_with_source      1601 non-null   float64\n",
      " 5   len_ratio_chars      1601 non-null   float64\n",
      " 6   len_ratio_words      1601 non-null   float64\n",
      " 7   abs_len_words        1601 non-null   int64  \n",
      " 8   flesch_reading_ease  1601 non-null   float64\n",
      " 9   avg_syl_per_word     1601 non-null   float64\n",
      " 10  sentence_count       1601 non-null   int64  \n",
      "dtypes: float64(6), int64(2), object(3)\n",
      "memory usage: 137.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Create the final DataFrame\n",
    "df = pd.DataFrame(all_records)\n",
    "\n",
    "# Display the first 5 rows\n",
    "df.head()\n",
    "\n",
    "# Display summary information about the DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc2a6a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success! Dataset saved to /clwork/kexin/tsar_st/proxy_training_data.csv\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"âœ… Success! Dataset saved to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c1c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tsar-st)",
   "language": "python",
   "name": "tsar-st"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
